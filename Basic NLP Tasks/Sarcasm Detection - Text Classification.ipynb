{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14985\n",
       "1    13634\n",
       "Name: is_sarcastic, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading in the data\n",
    "df=pd.read_json(r'C:\\Users\\MUKU\\Desktop\\Python\\NLP\\nlp datasets\\Sarcasm_Headlines_Dataset_v2.json',lines=True)\n",
    "\n",
    "# Checking if the data is balanced\n",
    "df['is_sarcastic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(doc):\n",
    "    doc=doc.lower()\n",
    "    tokens=[tokens for tokens in nlp(doc)]\n",
    "    tokens=[tokens for tokens in tokens if tokens.is_stop==False]\n",
    "    tokens=[tokens for tokens in tokens if tokens.is_punct==False]\n",
    "    tokens=' '.join(tokens.lemma_ for tokens in tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline_cleaned'] = df['headline'].apply(lambda x: cleanData(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X=df['headline']\n",
    "y=df['is_sarcastic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting and Tfidf\n",
    "tfidf_vect=TfidfVectorizer()\n",
    "X_train_tfidf=tfidf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the pipeline\n",
    "svc_pipeline=Pipeline([('tfidf',TfidfVectorizer()),('svc',LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and predicting\n",
    "svc_pipeline.fit(X_train,y_train)\n",
    "predictions=svc_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking scores and metrics\n",
    "def algorithm(algorithm):\n",
    "    pipeline=Pipeline([('tfidf',TfidfVectorizer()),('algo',algorithm)])\n",
    "    pipeline.fit(X_train,y_train)\n",
    "    predictions=pipeline.predict(X_test)\n",
    "    print(f'Accuracy Score is {round(accuracy_score(y_test,predictions),2)}')\n",
    "    print('\\n')\n",
    "    print(f'{confusion_matrix(y_test,predictions)}')\n",
    "    print('\\n')\n",
    "    print(f'{classification_report(y_test,predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd=SGDClassifier()\n",
    "svc=LinearSVC()\n",
    "rfc=RandomForestClassifier()\n",
    "clf=AdaBoostClassifier()\n",
    "list_of_algos=[sgd,svc,rfc,clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.83\n",
      "\n",
      "\n",
      "[[4057  859]\n",
      " [ 703 3826]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84      4916\n",
      "           1       0.82      0.84      0.83      4529\n",
      "\n",
      "    accuracy                           0.83      9445\n",
      "   macro avg       0.83      0.84      0.83      9445\n",
      "weighted avg       0.84      0.83      0.83      9445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algorithm(sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out new sentences\n",
    "X_test=pd.Series(['Light travels faster than sound. This is why some people appear bright until you hear them speak','It was then the lovable show met the leading inspection.','Its okay if you dont like me. Not everyone has good taste.','It was then the interesting appearance met the outlying lack.','You look good when your eyes are closed, but you look the best when my eyes are closed',\"The finished chemistry can't carry the club.\",'Mirrors cant talk, lucky for you they cant laugh either','What if the abundant permission ate the maximum?','If i had a dollar for every smart thing you say. i would be poor','The jagged character jails into the cloudy exercise.','Are you always so stupid or is today a special ocassion?','It was then the cooked boot met the wet shoe.','I feel so miserable without you, its almost like having you here','Did the scattered police really deserve the hurry?','If you find me offensive. Then i suggest you quit finding me'])\n",
    "y_test=[1,0,1,0,1,0,1,0,1,0,1,0,1,0,1]\n",
    "X_test=X_test.apply(lambda x: cleanData(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.73\n",
      "\n",
      "\n",
      "[[4 3]\n",
      " [1 7]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.67         7\n",
      "           1       0.70      0.88      0.78         8\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.75      0.72      0.72        15\n",
      "weighted avg       0.75      0.73      0.73        15\n",
      "\n",
      "Accuracy Score is 0.6\n",
      "\n",
      "\n",
      "[[4 3]\n",
      " [3 5]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57         7\n",
      "           1       0.62      0.62      0.62         8\n",
      "\n",
      "    accuracy                           0.60        15\n",
      "   macro avg       0.60      0.60      0.60        15\n",
      "weighted avg       0.60      0.60      0.60        15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MUKU\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.53\n",
      "\n",
      "\n",
      "[[1 6]\n",
      " [1 7]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.14      0.22         7\n",
      "           1       0.54      0.88      0.67         8\n",
      "\n",
      "    accuracy                           0.53        15\n",
      "   macro avg       0.52      0.51      0.44        15\n",
      "weighted avg       0.52      0.53      0.46        15\n",
      "\n",
      "Accuracy Score is 0.6\n",
      "\n",
      "\n",
      "[[1 6]\n",
      " [0 8]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.14      0.25         7\n",
      "           1       0.57      1.00      0.73         8\n",
      "\n",
      "    accuracy                           0.60        15\n",
      "   macro avg       0.79      0.57      0.49        15\n",
      "weighted avg       0.77      0.60      0.50        15\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[algorithm(algo) for algo in list_of_algos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
