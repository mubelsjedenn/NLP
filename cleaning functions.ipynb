{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from time import time, sleep\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "random.seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed opinion loading time: 1.4 minutes\n",
      "Elapsed cluster loading time: 1.4 minutes\n",
      "Total html parsing time: 1.5 minutes\n",
      "After parsing html, there are 0 empty opinions remaining\n",
      "\n",
      "\n",
      "***SANITY CHECK {}***: \n",
      " CASE NAME: Automobile Workers v. Johnson Controls, Inc. \n",
      " CASE DATE: 1991-03-20 00:00:00 \n",
      " \n",
      " CASE TEXT:\n",
      " 499 U.S. 187 (1991)\n",
      "INTERNATIONAL UNION, UNITED AUTOMOBILE, AEROSPACE & AGRICULTURAL IMPLEMENT WORKERS OF AMERICA, UAW, ET AL.\n",
      "v.\n",
      "JOHNSON CONTROLS, INC.\n",
      "No. 89-1215.\n",
      "Supreme Court of the United States.\n",
      "Argued October 10, 1990.\n",
      "Decided March 20, 1991.\n",
      "CERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE SEVENTH CIRCUIT.\n",
      "*189 Marsha S. Berzon argued the cause for petitioners. With her on the briefs were Jordan Rossen, Ralph O. Jones, and Laurence Gold.\n",
      "Stanley S. Jaspan argued the cause for re\n",
      "\n",
      "\n",
      "***SANITY CHECK {}***: \n",
      " CASE NAME: Birdsell v. Shaliol \n",
      " CASE DATE: 1884-12-08 00:00:00 \n",
      " \n",
      " CASE TEXT:\n",
      " 112 U.S. 485 (1884)\n",
      "BIRDSELL & Others\n",
      "v.\n",
      "SHALIOL & Another.\n",
      "Supreme Court of United States.\n",
      "Argued November 12, 1884.\n",
      "Decided December 8, 1884.\n",
      "APPEAL FROM THE CIRCUIT COURT OF THE UNITED STATES FOR THE NORTHERN DISTRICT OF OHIO.\n",
      "*486 Mr. W.W. Leggett (Mr. M.D. Leggett was with him) for appellants.\n",
      "No appearance for appellees.\n",
      "MR. JUSTICE GRAY delivered the opinion of the court. He recited the facts as above stated, and continued:\n",
      "The plaintiffs in the present suit, Birdsell, the patentee, in wh\n",
      "\n",
      "\n",
      "***SANITY CHECK {}***: \n",
      " CASE NAME: Hitz v. National Metropolitan Bank \n",
      " CASE DATE: 1884-05-05 00:00:00 \n",
      " \n",
      " CASE TEXT:\n",
      " 111 U.S. 722 (1884)\n",
      "HITZ\n",
      "v.\n",
      "NATIONAL METROPOLITAN BANK.\n",
      "Supreme Court of United States.\n",
      "Argued March 14th and 17th, 1884.\n",
      "Decided May 5th, 1884.\n",
      "APPEAL FROM THE SUPREME COURT OF THE DISTRICT OF COLUMBIA.\n",
      "*723 Mr. Enoch Totten and Mr. R.D. Mussey for appellant.\n",
      "Mr. Leigh Robinson for appellee.\n",
      "MR. JUSTICE MILLER delivered the opinion of the court.\n",
      "This is a bill in chancery brought by the Bank against John Hitz, Jane C. Hitz, his wife, and Metzerott and Cross, trustees, to declare void a deed, so\n"
     ]
    }
   ],
   "source": [
    "# IMPORT JSONS\n",
    "import os\n",
    "import glob\n",
    "from lxml import html\n",
    "\n",
    "start = time()\n",
    "jsons_as_series = []\n",
    "file_list = glob.glob('data/scotus_opinions/*.json')\n",
    "\n",
    "for filename in file_list:\n",
    "    with open(filename) as json_data:\n",
    "        json_1 = json.load(json_data)\n",
    "        jsons_as_series.append(pd.Series(json_1))\n",
    "\n",
    "scotus_df = pd.DataFrame(jsons_as_series)\n",
    "print(\"Elapsed opinion loading time:\", round((time()-start)/60, 1), 'minutes')\n",
    "\n",
    "\n",
    "# REMOVE DISMISSALS (coextensive with non-per-curiam, short texts with no majority opinion) \n",
    "# -- mostly denial of certiorari, but some misc. dismissals\n",
    "scotus_df['per_curiam'] = scotus_df.per_curiam.astype(bool)\n",
    "dismissals_index = scotus_df[\n",
    "    (~scotus_df.per_curiam)\n",
    "    & (scotus_df.html_with_citations.map(lambda x: len(x) < 5000))\n",
    "    & (scotus_df.html_with_citations.map(lambda x: x.lower().find('delivered the opinion of the court.') == -1))\n",
    "].index\n",
    "scotus_df = scotus_df.drop(dismissals_index)\n",
    "\n",
    "# LOAD AND LINK CLUSTERS\n",
    "# first, convert all http URLs to https (we'll need this for consistency of merging, and user convenience)\n",
    "def to_https(url):\n",
    "    if url[:5] != 'https':\n",
    "        url = 'https' + url[4:]\n",
    "    if url[:32] == 'https://www.courtlistener.com:80': # fix erroneous :80 urls\n",
    "        url = 'https://www.courtlistener.com' + url[32:]\n",
    "    return url\n",
    "\n",
    "scotus_df['cluster'] = scotus_df['cluster'].map(to_https)\n",
    "\n",
    "start = time()\n",
    "jsons_as_series = []\n",
    "file_list = glob.glob('data/scotus_clusters/*.json')\n",
    "\n",
    "for filename in file_list:\n",
    "    with open(filename) as json_data:\n",
    "        json_1 = json.load(json_data)\n",
    "        jsons_as_series.append(pd.Series(json_1))\n",
    "\n",
    "clusters_df = pd.DataFrame(jsons_as_series)\n",
    "clusters_df['resource_uri'] = clusters_df.resource_uri.map(to_https)\n",
    "print(\"Elapsed cluster loading time:\", round((time()-start)/60, 1), 'minutes')\n",
    "\n",
    "# merge info from clusters_df into opinions_df\n",
    "cases_df = pd.merge(scotus_df, \n",
    "                       clusters_df[['case_name',\n",
    "                                    'date_filed',\n",
    "                                    'federal_cite_one', \n",
    "                                    'resource_uri',\n",
    "                                    'scdb_id',\n",
    "                                    'scdb_decision_direction',\n",
    "                                    'scdb_votes_majority',\n",
    "                                    'scdb_votes_minority'\n",
    "                                   ]], \n",
    "                       how='left', \n",
    "                       left_on='cluster', \n",
    "                       right_on='resource_uri')\n",
    "\n",
    "# winnow down to the relevant columns (note: we'll drop the few cases of plain_text for consistency's sake)\n",
    "cases_df = cases_df[[\n",
    "    'case_name',\n",
    "    'author_str',\n",
    "    'date_filed',  \n",
    "    'federal_cite_one',\n",
    "    'per_curiam',\n",
    "    'author',  \n",
    "    'cluster',\n",
    "    'absolute_url',\n",
    "    'html_with_citations',\n",
    "    'scdb_id',\n",
    "    'scdb_decision_direction',\n",
    "    'scdb_votes_majority',\n",
    "    'scdb_votes_minority'\n",
    "]]\n",
    "\n",
    "# PARSE HTML\n",
    "start = time()\n",
    "cases_df['html_with_citations'] = cases_df.html_with_citations.astype(str)\n",
    "cases_df = cases_df[cases_df.html_with_citations.map(lambda x: len(x) > 1)] # eliminate one empty string\n",
    "cases_df['absolute_url'] = 'https://www.courtlistener.com' + cases_df.absolute_url\n",
    "def extract_text(raw_html):\n",
    "    return html.fromstring(raw_html).text_content().strip()\n",
    "cases_df['plain_text'] = cases_df.html_with_citations.map(lambda x: extract_text(x))\n",
    "is_empty_now = cases_df.plain_text.isnull()\n",
    "print('Total html parsing time:', round((time()-start)/60, 1), 'minutes')\n",
    "print(\"After parsing html, there are {} empty opinions remaining\".format(sum(is_empty_now)))\n",
    "cases_df = cases_df[~cases_df.per_curiam.isnull()]\n",
    "\n",
    "# remove remaining certiorari and misc. non-decisions: no listed decision direction, and no majority opinion\n",
    "non_decision_index = cases_df[(~cases_df.per_curiam) \n",
    "         & (cases_df.scdb_decision_direction.isnull())\n",
    "         & (cases_df.plain_text.map(\n",
    "             lambda x: x.lower().find('delivered the opinion of the court.')==-1))\n",
    "        ].index\n",
    "cases_df = cases_df.drop(non_decision_index)\n",
    "\n",
    "# remove duplicate cases\n",
    "cases_df = cases_df.drop_duplicates(subset='federal_cite_one')\n",
    "\n",
    "# convert dates to datetime\n",
    "import datetime\n",
    "cases_df['date_filed'] = pd.to_datetime(cases_df.date_filed)\n",
    "cases_df['year_filed'] = cases_df.date_filed.map(lambda x: x.year)\n",
    "cases_df['year_filed'] = cases_df.year_filed.astype(int)\n",
    "# filter by date here if desired:\n",
    "# cases_df = cases_df[cases_df.year_filed >= 1970]\n",
    "\n",
    "# SANITY CHECK: do dates and titles match texts?\n",
    "checks = [83,1065,4508]\n",
    "for c in checks:\n",
    "    i = cases_df.index[c]\n",
    "    print(\n",
    "        '\\n\\n***SANITY CHECK {}***: \\n',\n",
    "        'CASE NAME:', cases_df.case_name[i], '\\n',\n",
    "        'CASE DATE:', cases_df.date_filed[i], '\\n', '\\n',\n",
    "        'CASE TEXT:\\n', cases_df.plain_text[i][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing text into opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed opinion parsing time: 23.2 minutes     \n"
     ]
    }
   ],
   "source": [
    "# PARSE plain text into separate opinions\n",
    "def find_author_listed_before(text, index):\n",
    "    '''\n",
    "    Returns first justice name preceding INDEX in the same sentence of TEXT.  If no justice named \n",
    "    between INDEX and the end of the previous sentence, returns None.\n",
    "    '''\n",
    "    text = text[:index].lower().replace('mr.','mr ')\n",
    "    start_index = text.rfind(\".\")\n",
    "    sentence = text[start_index:]\n",
    "    \n",
    "    justice_index = sentence.find(\"justice \")\n",
    "    if justice_index == -1:\n",
    "        justice_index = sentence.find(\"justice\\n\")\n",
    "        if justice_index == -1:\n",
    "            # catch rare format \"Smith, Justice, delivered the opinion of the court.\"\n",
    "            justice_index = sentence.find(\"justice, delivered\")\n",
    "            if justice_index != -1:\n",
    "                return \"justice \" + sentence[:justice_index].split()[-1][:-1] # name is prev word sans comma\n",
    "    if justice_index == -1:\n",
    "        return None\n",
    "\n",
    "    name_words = sentence[justice_index:].split()[:2]\n",
    "    name_words[-1] = name_words[-1].replace(',','') # remove trailing comma if present\n",
    "    name = \" \".join(name_words)\n",
    "    if name == 'justice dissentin': # catch rare false flag (actually a citation)\n",
    "        return None\n",
    "    return name\n",
    "\n",
    "def get_index_from_keyphrase(text, start_index, keyphrase, alternate_keyphrase=None):\n",
    "    '''\n",
    "    returns first index of KEYPHRASE(str) in TEXT[START_INDEX:] that has an author name \n",
    "    preceding it in the same sentence; returns None if none found\n",
    "    '''\n",
    "    search_text = text[start_index:]\n",
    "    index = search_text.find(keyphrase)\n",
    "    # if there isn't a justice preceding the keyphrase in the same sentence (rare),\n",
    "    # then this is a false flag.  Move on to the next occurrence of the keyphrase and repeat until true flag or end.\n",
    "    while index != -1 and find_author_listed_before(search_text, index + len(keyphrase)-2) is None:\n",
    "        new_index = search_text[(index + len(keyphrase)):].find(keyphrase)\n",
    "        index = new_index if new_index == -1 else new_index + (index + len(keyphrase))\n",
    "        # because the search started with the index of the prev find as 0\n",
    "    if index != -1:\n",
    "        index += len(keyphrase) + start_index\n",
    "    elif alternate_keyphrase is not None:\n",
    "        index = get_index_from_keyphrase(text, start_index, alternate_keyphrase, None)\n",
    "    return index\n",
    "\n",
    "def get_indices(text, per_curiam=False):\n",
    "    ''' \n",
    "    returns dictionary of beginning indices of majority / concurring / dissenting opinions in TEXT\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    indices = {}\n",
    "    bookmark = 0  # keeps track of where to start our next search\n",
    "    \n",
    "    if per_curiam:\n",
    "        indices['majority'] = text.find(\"per curiam.\")\n",
    "        if indices['majority'] != -1:\n",
    "            indices['majority'] += len(\"per curiam.\")\n",
    "    else:\n",
    "        indices['majority'] = get_index_from_keyphrase(text, 0, 'delivered the opinion of the court.', 'join.')\n",
    "\n",
    "    if indices['majority'] == -1: # short-circuit if there is no majority opinion: it's a dismissal (or an anomaly)\n",
    "        return indices\n",
    "    \n",
    "    bookmark = indices['majority']\n",
    "            \n",
    "    indices['first_concurring'] = get_index_from_keyphrase(\n",
    "        text,\n",
    "        bookmark,\n",
    "        'concurring.',\n",
    "        'concurring in the judgment.'\n",
    "    )\n",
    "    bookmark = max(bookmark, indices['first_concurring'])\n",
    "    \n",
    "    if indices['first_concurring'] == -1:\n",
    "        indices['second_concurring'] = -1\n",
    "    else:\n",
    "        indices['second_concurring'] = get_index_from_keyphrase(\n",
    "            text,\n",
    "            bookmark,\n",
    "            'concurring.'\n",
    "        )\n",
    "        bookmark = max(bookmark, indices['second_concurring'])\n",
    "    \n",
    "    indices['first_dissenting'] = get_index_from_keyphrase(\n",
    "        text,\n",
    "        bookmark,\n",
    "        'dissenting.'\n",
    "    )\n",
    "    bookmark = max(bookmark, indices['first_dissenting'])\n",
    "\n",
    "    \n",
    "    if indices['first_dissenting'] == -1:\n",
    "        indices['second_dissenting'] = -1\n",
    "    else:\n",
    "        indices['second_dissenting'] = get_index_from_keyphrase(\n",
    "            text,\n",
    "            bookmark,\n",
    "            'dissenting.'\n",
    "        )\n",
    "\n",
    "    return indices\n",
    "\n",
    "def remove_next_intro(text):\n",
    "    '''removes last sentence of text if it's introducing the next opinion '''\n",
    "    if text[-11:] in ['concurring.', 'dissenting.']:\n",
    "        end_of_prev_sentence = text[:-1].replace('Mr.','Mr ').rfind('.')\n",
    "        text = text[:end_of_prev_sentence + 2] # +2 to include last char and period\n",
    "    return text\n",
    "\n",
    "def split_and_label(text, per_curiam=False, include_concurring=True, include_second_dissent=True):\n",
    "    ''' returns a list of tuples formatted as (author, majority/concurring/dissenting, text)'''\n",
    "    opinions = []\n",
    "    indices = get_indices(text, per_curiam)\n",
    "    \n",
    "    if indices['majority'] == -1: # indicates empty / dismissal / haywire\n",
    "        return [None]\n",
    "    \n",
    "    majority_endpoint = indices['first_concurring'] if indices['first_concurring'] != -1 \\\n",
    "                            else indices['first_dissenting']\n",
    "    if per_curiam:\n",
    "        majority = (\n",
    "            'per_curiam',\n",
    "            'per_curiam',\n",
    "            remove_next_intro( text[indices['majority']:majority_endpoint] ).strip()\n",
    "        ) \n",
    "    else:\n",
    "        majority = (\n",
    "            find_author_listed_before(text, indices['majority']-1), # -1 to avoid including final period (find_author)\n",
    "            'majority',\n",
    "            remove_next_intro( text[indices['majority']:majority_endpoint] ).strip()\n",
    "        )\n",
    "    opinions.append(majority)\n",
    "    \n",
    "    concurring_endpoint = indices['second_concurring'] if indices['second_concurring'] != -1 \\\n",
    "                            else indices['first_dissenting']\n",
    "    if include_concurring and indices['first_concurring'] != -1:\n",
    "        first_concurring = (\n",
    "            find_author_listed_before(text, indices['first_concurring']-1),\n",
    "            'concurring',\n",
    "            remove_next_intro( text[indices['first_concurring']:concurring_endpoint] ).strip()\n",
    "        )\n",
    "        opinions.append(first_concurring)\n",
    "        \n",
    "    if indices['first_dissenting'] != -1:\n",
    "        first_dissenting = (\n",
    "            find_author_listed_before(text, indices['first_dissenting']-1),\n",
    "            'dissenting',\n",
    "            remove_next_intro( text[indices['first_dissenting']:indices['second_dissenting']] ).strip()\n",
    "        )\n",
    "        opinions.append(first_dissenting)\n",
    "        \n",
    "    if include_second_dissent and indices['second_dissenting'] != -1:\n",
    "        second_dissenting = (\n",
    "            find_author_listed_before(text, indices['second_dissenting']-1),\n",
    "            'second_dissenting',\n",
    "            remove_next_intro( text[indices['second_dissenting']:] ).strip()\n",
    "        )\n",
    "        opinions.append(second_dissenting)\n",
    "        \n",
    "    # clip \"notes\" section from end of the text of the last opinion in the case file\n",
    "    notes_index = opinions[-1][2].find('NOTES')\n",
    "    if notes_index == -1:\n",
    "        notes_index = opinions[-1][2].find('APPENDIXES')\n",
    "    if notes_index != -1:\n",
    "        opinions[-1] = (opinions[-1][0], \n",
    "                        opinions[-1][1], \n",
    "                        opinions[-1][2][:notes_index])\n",
    "        \n",
    "    return opinions\n",
    "\n",
    "columns = [\n",
    "    'author_name',\n",
    "    'category',\n",
    "    'per_curiam',\n",
    "    'case_name',\n",
    "    'date_filed',\n",
    "    'federal_cite_one',\n",
    "    'absolute_url',\n",
    "    'cluster',\n",
    "    'year_filed',\n",
    "    'scdb_id',\n",
    "    'scdb_decision_direction',\n",
    "    'scdb_votes_majority',\n",
    "    'scdb_votes_minority',\n",
    "    'text'\n",
    "]\n",
    "opinions_df = pd.DataFrame(columns=columns)\n",
    "counter = 0\n",
    "start = time()\n",
    "\n",
    "# .drop_duplicates(subset='federal_cite_one')\n",
    "for i in cases_df.index:\n",
    "    counter += 1\n",
    "    print(\"Processing row {} of {}\".format(counter, cases_df.shape[0]), end='\\r')\n",
    "    text = cases_df.plain_text[i]\n",
    "    per_curiam = cases_df.per_curiam[i]\n",
    "    opinions = split_and_label(text, per_curiam)\n",
    "    if opinions[0] is None: # if no majority opinion, either empty or something is haywire \n",
    "        continue\n",
    "    for opinion in opinions:\n",
    "        new_row = pd.Series(\n",
    "            [           \n",
    "                opinion[0], # author\n",
    "                opinion[1], # majority/concurring/dissenting\n",
    "                per_curiam,\n",
    "                cases_df.case_name[i],\n",
    "                cases_df.date_filed[i],\n",
    "                cases_df.federal_cite_one[i],\n",
    "                cases_df.absolute_url[i],\n",
    "                cases_df.cluster[i],\n",
    "                cases_df.year_filed[i],\n",
    "                cases_df.scdb_id[i],\n",
    "                cases_df.scdb_decision_direction[i],\n",
    "                cases_df.scdb_votes_majority[i],\n",
    "                cases_df.scdb_votes_minority[i],\n",
    "                opinion[2] # text\n",
    "            ],\n",
    "        index=columns)\n",
    "        \n",
    "#         print(new_row[:-1])\n",
    "        opinions_df.loc[opinions_df.shape[0]] = new_row # append without creating new object each time\n",
    "    \n",
    "print(\"Elapsed opinion parsing time:\", round((time()-start)/60, 1), 'minutes     ')\n",
    "\n",
    "# retyping as necessary\n",
    "opinions_df.per_curiam = opinions_df.per_curiam.astype(bool)\n",
    "opinions_df.year_filed = opinions_df.year_filed.astype(int)\n",
    "\n",
    "# drop any blank opinions that got read in (very few - about 7)\n",
    "opinions_df = opinions_df[opinions_df.text.map(lambda x: len(x) > 1)]\n",
    "\n",
    "# resolve apostrophe format discrepancies\n",
    "opinions_df.author_name = opinions_df.author_name.map(lambda x: x.replace('’','\\''))\n",
    "opinions_df.author_name = opinions_df.author_name.map(lambda x: x.replace('`','\\''))\n",
    "\n",
    "# remove very rare (mostly erroneous) author_name values if desired:\n",
    "# rare_authors = list(opinions_df.author_name.value_counts()[opinions_df.author_name.value_counts() <= 5].index)\n",
    "# opinions_df = opinions_df[~opinions_df.author_name.isin(rare_authors)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
